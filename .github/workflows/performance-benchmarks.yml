name: Performance Benchmarks

on:
  pull_request:
    paths:
      - 'lib/leyline/commands/**'
      - 'lib/leyline/cache/**'
      - 'lib/leyline/discovery/**'
      - 'spec/benchmarks/**'
      - '.github/workflows/performance-benchmarks.yml'
  push:
    branches:
      - master
  schedule:
    # Run nightly to catch performance drift
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      verbose:
        description: 'Enable verbose output'
        required: false
        default: 'false'

jobs:
  performance-validation:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        ruby: ['3.0', '3.1', '3.2']

    name: Benchmarks - ${{ matrix.os }} / Ruby ${{ matrix.ruby }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history for baseline comparison

      - name: Setup Ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: ${{ matrix.ruby }}
          bundler-cache: true

      - name: Validate Bundler Platform Compatibility
        run: |
          echo "ðŸ” Validating Bundler platform compatibility"
          echo "Current runner platform: ${{ runner.os }}-${{ runner.arch }}"

          # Map GitHub Actions runner info to expected Gemfile.lock platforms
          case "${{ runner.os }}" in
            "Linux")
              expected_platform="x86_64-linux"
              ;;
            "macOS")
              if [[ "${{ runner.arch }}" == "ARM64" ]]; then
                expected_platform="arm64-darwin-23"
              else
                expected_platform="x86_64-darwin-23"
              fi
              ;;
            "Windows")
              # Ruby 3.1+ uses x64-mingw-ucrt, Ruby 3.0 uses x64-mingw32
              if [[ "${{ matrix.ruby }}" == "3.0" ]]; then
                expected_platform="x64-mingw32"
              else
                expected_platform="x64-mingw-ucrt"
              fi
              ;;
            *)
              echo "::error::Unknown runner OS: ${{ runner.os }}"
              exit 1
              ;;
          esac

          echo "Expected platform in Gemfile.lock: $expected_platform"

          # Check if the expected platform exists in Gemfile.lock
          if ! grep -q "$expected_platform" Gemfile.lock; then
            echo "::error::Platform compatibility issue detected!"
            echo "::error::Gemfile.lock does not support platform: $expected_platform"
            echo "::error::"
            echo "::error::Current platforms in Gemfile.lock:"
            grep -A 10 "^PLATFORMS" Gemfile.lock || echo "::error::No PLATFORMS section found"
            echo "::error::"
            echo "::error::To fix this issue, run one of the following commands locally:"
            echo "::error::  bundle lock --add-platform $expected_platform"
            echo "::error::  bundle lock --add-platform x86_64-linux x86_64-darwin-23 arm64-darwin-23 x64-mingw32"
            echo "::error::"
            echo "::error::Then commit and push the updated Gemfile.lock"
            exit 1
          fi

          echo "âœ… Platform $expected_platform found in Gemfile.lock"
          echo "ðŸŽ¯ Bundler platform compatibility validated successfully"
        shell: bash

      # Windows diagnostic removed - platform issues resolved
      # - name: Windows Platform Diagnostic
      #   if: runner.os == 'Windows'
      #   run: |
      #     echo "ðŸ” Running Windows Platform Diagnostic"
      #     ruby windows-diagnostic.rb
      #   shell: bash

      - name: Setup benchmark environment
        run: |
          # Create directories
          mkdir -p benchmark-results
          mkdir -p .benchmarks

          # Set environment variables
          echo "LEYLINE_BENCHMARK_MODE=true" >> $GITHUB_ENV
          echo "LEYLINE_CACHE_DIR=${{ runner.temp }}/leyline-bench-cache" >> $GITHUB_ENV
        shell: bash

      - name: Download baseline benchmarks
        uses: actions/download-artifact@v4
        with:
          name: benchmark-baseline-${{ matrix.os }}
          path: .benchmarks
        continue-on-error: true  # First run won't have baseline

      - name: Run micro benchmarks
        run: |
          bundle exec rspec spec/benchmarks/micro_benchmarks_spec.rb \
            --format json \
            --out benchmark-results/micro.json \
            --format documentation
        timeout-minutes: 10

      - name: Run macro benchmarks
        run: |
          bundle exec rspec spec/benchmarks/macro_benchmarks_spec.rb \
            --format json \
            --out benchmark-results/macro.json \
            --format documentation
        timeout-minutes: 15

      - name: Run degradation tests
        run: |
          bundle exec rspec spec/benchmarks/degradation_benchmarks_spec.rb \
            --format json \
            --out benchmark-results/degradation.json \
            --format documentation
        timeout-minutes: 10

      - name: Generate benchmark report
        run: |
          # Combine all results
          ruby -e "
            require 'json'
            results = {}
            Dir.glob('benchmark-results/*.json').each do |file|
              data = JSON.parse(File.read(file))
              results.merge!(data) if data.is_a?(Hash)
            end
            results['metadata'] = {
              'timestamp' => Time.now.iso8601,
              'os' => '${{ matrix.os }}',
              'ruby_version' => '${{ matrix.ruby }}',
              'commit' => ENV['GITHUB_SHA'],
              'branch' => ENV['GITHUB_REF_NAME']
            }
            File.write('benchmark-results/combined.json', JSON.pretty_generate(results))
          "

      - name: Compare with baseline
        if: github.event_name == 'pull_request'
        run: |
          if [ -f ".benchmarks/baseline.json" ]; then
            bundle exec ruby tools/compare_benchmarks.rb \
              --baseline .benchmarks/baseline.json \
              --current benchmark-results \
              --threshold 0.1 \
              --format markdown > benchmark-comparison.md

            # Save exit code
            echo "BENCHMARK_COMPARISON_EXIT_CODE=$?" >> $GITHUB_ENV
          else
            echo "No baseline found - this will become the new baseline"
            echo "BENCHMARK_COMPARISON_EXIT_CODE=0" >> $GITHUB_ENV
          fi
        shell: bash
        continue-on-error: true

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.os }}-${{ matrix.ruby }}
          path: benchmark-results/
          retention-days: 30

      - name: Update baseline (master branch only)
        if: github.ref == 'refs/heads/master' && github.event_name == 'push'
        run: |
          cp benchmark-results/combined.json .benchmarks/baseline.json

          # Upload as artifact for future runs
          mkdir -p baseline-upload
          cp .benchmarks/baseline.json baseline-upload/
        shell: bash

      - name: Upload baseline artifact
        if: github.ref == 'refs/heads/master' && github.event_name == 'push'
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-baseline-${{ matrix.os }}
          path: baseline-upload/baseline.json
          retention-days: 90

      - name: Comment on PR
        if: github.event_name == 'pull_request' && matrix.os == 'ubuntu-latest' && matrix.ruby == '3.2'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Read comparison report
            let comment = '## ðŸ“Š Performance Benchmark Results\n\n';

            if (fs.existsSync('benchmark-comparison.md')) {
              comment += fs.readFileSync('benchmark-comparison.md', 'utf8');
            } else {
              comment += 'âœ… First benchmark run - no baseline for comparison\n';
            }

            // Add run details
            comment += '\n\n---\n';
            comment += `*Run on ${process.env.RUNNER_OS} with Ruby ${{ matrix.ruby }}*\n`;
            comment += `*Commit: ${process.env.GITHUB_SHA.substring(0, 7)}*`;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Performance Benchmark Results')
            );

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      - name: Performance gate check
        if: github.event_name == 'pull_request'
        run: |
          if [ "$BENCHMARK_COMPARISON_EXIT_CODE" != "0" ]; then
            echo "âŒ Performance regressions detected!"
            echo "See benchmark comparison report above for details."
            exit 1
          else
            echo "âœ… Performance benchmarks passed!"
          fi
        shell: bash

  cross-platform-summary:
    needs: performance-validation
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          path: all-benchmarks

      - name: Generate cross-platform summary
        run: |
          echo "# Cross-Platform Performance Summary" > summary.md
          echo "" >> summary.md
          echo "| Platform | Ruby | Status | Avg Response Time | Memory Usage |" >> summary.md
          echo "|----------|------|--------|-------------------|--------------|" >> summary.md

          # Process results from each platform
          for dir in all-benchmarks/benchmark-results-*; do
            if [ -d "$dir" ]; then
              platform=$(basename "$dir" | cut -d- -f3)
              ruby=$(basename "$dir" | cut -d- -f4)

              # Extract key metrics (simplified for example)
              if [ -f "$dir/combined.json" ]; then
                echo "| $platform | $ruby | âœ… | TBD ms | TBD MB |" >> summary.md
              else
                echo "| $platform | $ruby | âŒ | N/A | N/A |" >> summary.md
              fi
            fi
          done

          cat summary.md

      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: performance-summary
          path: summary.md
